{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401bd3fb-5582-4747-8516-e22371fbc326",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ml_project/.venv/lib/python3.9/site-packages/mlflow/utils/requirements_utils.py:12: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf28cedf-0059-4fbd-94d4-7aa147c443d9",
   "metadata": {},
   "source": [
    "# DAG BORRAR DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5151d029-1480-467d-a60b-ba1bbbdde7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Todas las tablas de raw han sido eliminadas!\n",
      "¡Todas las tablas de clean han sido eliminadas!\n",
      "Se reinicia data\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "import pymysql\n",
    "\n",
    "\n",
    "#Borrar tablas de DB\n",
    "engine1 = create_engine(\"mysql+pymysql://user:password@db-raw-data:3307/db\")\n",
    "\n",
    "meta1 = MetaData()\n",
    "meta1.reflect(bind=engine1)\n",
    "meta1.drop_all(bind=engine1)\n",
    "print(\"¡Todas las tablas de raw han sido eliminadas!\")\n",
    "\n",
    "engine2 = create_engine(\"mysql+pymysql://user:password@db-clean-data:3308/db\")\n",
    "\n",
    "meta2 = MetaData()\n",
    "meta2.reflect(bind=engine2)\n",
    "meta2.drop_all(bind=engine2)\n",
    "print(\"¡Todas las tablas de clean han sido eliminadas!\")\n",
    "\n",
    "\n",
    "# Hacer la petición\n",
    "url = \"http://10.43.101.108:80/restart_data_generation\"\n",
    "params = {\n",
    "    \"group_number\": 2,\n",
    "    \"day\": \"Wednesday\"\n",
    "}\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Convertimos la respuesta en DataFrame directamente\n",
    "    print(\"Se reinicia data\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6f7ab-49c4-4f7c-a8ab-8ed3300ef4b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DAG PRINCIPAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52b2d8-ba50-47e8-9f43-682c6a0577e9",
   "metadata": {},
   "source": [
    "## TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e1c4cd3b-71f2-4640-aa9d-0b08a0374035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "\n",
    "# Hacer la petición\n",
    "url = \"http://10.43.101.108:80/data\"\n",
    "params = {\"group_number\": 2, \"day\": \"Wednesday\"}\n",
    "headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, params=params, headers=headers)\n",
    "engine = create_engine(\"mysql+pymysql://user:password@db-raw-data:3307/db\")\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Convertimos la respuesta en DataFrame directamente\n",
    "    df_raw = pd.DataFrame(response.json())\n",
    "    # Normalizamos la columna 'data'\n",
    "    df_data = pd.json_normalize(df_raw[\"data\"])\n",
    "    # Combinamos con las columnas del DataFrame original (excepto 'data')\n",
    "    df = pd.concat([df_raw.drop(columns=[\"data\"]), df_data], axis=1)\n",
    "    #Remover duplicados\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # Inserta el DataFrame como una tabla (esto reemplaza si ya existe)\n",
    "    df.to_sql(\"data_raw\", con=engine, index=False, if_exists=\"append\")\n",
    "else:\n",
    "    print(f\"Error en la petición: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44cea2-87ac-45d4-b3e4-8a5913d23648",
   "metadata": {},
   "source": [
    "## TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1250dee-e2b0-4d0c-b3b7-e8b81ca4e33f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocesamiento completo. Tablas 'train_data' y 'test_data' creadas en MySQL.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "\n",
    "engine1 = create_engine(\"mysql+pymysql://user:password@db-raw-data:3307/db\")\n",
    "df = pd.read_sql_table(\"data_raw\", engine1)\n",
    "\n",
    "df.drop(columns=['group_number','day','status','street','city','state','zip_code','prev_sold_date'], inplace=True)\n",
    "df = df[df['batch_number'] == df['batch_number'].max()].dropna()\n",
    "    \n",
    "# Selección de variables predictoras y objetivo\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "\n",
    "# Dividir en entrenamiento y prueba (80%-20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Guardar los conjuntos como tablas SQL (reemplazando si existen)\n",
    "engine2 = create_engine(\"mysql+pymysql://user:password@db-clean-data:3308/db\")\n",
    "X_train.to_sql(\"train_data_X\", con=engine2, if_exists=\"append\", index=False)\n",
    "X_test.to_sql(\"test_data_X\", con=engine2, if_exists=\"append\", index=False)\n",
    "y_train.to_sql(\"train_data_y\", con=engine2, if_exists=\"append\", index=False)\n",
    "y_test.to_sql(\"test_data_y\", con=engine2, if_exists=\"append\", index=False)\n",
    "\n",
    "print(\"Preprocesamiento completo. Tablas 'train_data' y 'test_data' creadas en MySQL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40834fc3-040a-4324-a548-b287b84b3787",
   "metadata": {},
   "source": [
    "## TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "79c6a4b8-b276-4e8d-841a-29d57719e205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install scipy\n",
    "#!pip install matplotlib\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4dede058-1d78-4152-bc2f-0d7724a17dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/06/02 20:58:26 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/06/02 20:58:26 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of statsmodels. If you encounter errors during autologging, try upgrading / downgrading statsmodels to a supported version, or try upgrading MLflow.\n",
      "2025/06/02 20:58:26 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/06/02 20:58:28 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/ml_project/.venv/lib/python3.9/site-packages/mlflow/models/signature.py:137: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "Successfully registered model 'Regresion_Lineal_modelo_produccion'.\n",
      "2025/06/02 20:58:43 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: Regresion_Lineal_modelo_produccion, version 1\n",
      "Created version '1' of model 'Regresion_Lineal_modelo_produccion'.\n",
      "/tmp/ipykernel_29/3733881917.py:162: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  Validations = pd.concat([Validations, nueva_fila], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "from scipy.stats import ks_2samp, wasserstein_distance\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from mlflow.tracking import MlflowClient\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://user:password@db-clean-data:3308/db\")\n",
    "\n",
    "# Separar variables predictoras y objetivo\n",
    "X_train = pd.read_sql_table(\"train_data_X\", engine)\n",
    "y_train = pd.read_sql_table(\"train_data_y\", engine).values.ravel()\n",
    "X_test = pd.read_sql_table(\"test_data_X\", engine)\n",
    "y_test = pd.read_sql_table(\"test_data_y\", engine).values.ravel()\n",
    "\n",
    "max_batch = int(X_train['batch_number'].max())\n",
    "\n",
    "Validations = pd.DataFrame(columns=[\"batch_number\", \"Validaciones-hechas\", \"Despliegue_de_nuevo_modelo\", \"Fecha\"])\n",
    "\n",
    "training_flag = False\n",
    "shift_detected = False\n",
    "\n",
    "if max_batch==0:\n",
    "    text_validaciones = \"No se hacen validaciones ya que son los primeros datos. Se entrena el primer modelo de todos.\"\n",
    "    training_flag = True\n",
    "else:\n",
    "    X_train_last = X_train[X_train['batch_number'] == max_batch].drop(columns=['batch_number'])    # filas con batch máximo\n",
    "    X_train_rest = X_train[X_train['batch_number'] != max_batch].drop(columns=['batch_number'])    # filas con el resto del dataframe\n",
    "    \n",
    "    ratio = len(X_train_last) / len(X_train_rest)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_rest_scaled = pd.DataFrame(scaler.fit_transform(X_train_rest), columns=X_train_rest.columns)\n",
    "    X_train_last_scaled = pd.DataFrame(scaler.transform(X_train_last), columns=X_train_last.columns)\n",
    "\n",
    "    \n",
    "    if ratio <0.05:\n",
    "        text_validaciones = f\"Los nuevos datos representan menos del 5% de la data total, exactamente un {ratio*100}%. No se hacen más validaciones. Ni se entrena nuevo modelo.\"\n",
    "        test_despliegue = \"No hay nuevo modelo para desplegar\"\n",
    "    else:\n",
    "        variables_con_drift = []\n",
    "        if len(X_train_last) > len(X_train_rest):\n",
    "            X_train_last=X_train_last.sample(len(X_train_rest), random_state=42)\n",
    "        else:\n",
    "            X_train_rest=X_train_rest.sample(len(X_train_last), random_state=42)\n",
    "            \n",
    "            \n",
    "        for col in X_train_rest.columns:\n",
    "            #stat, p_value  = ks_2samp(X_train_last[col], X_train_rest[col])\n",
    "            #Print(KS)\n",
    "            #if p_value < 0.05:\n",
    "            #    shift_detected = True\n",
    "            #    training_flag = True\n",
    "            #    variables_con_drift.append(f\"Se detecta data drift en la variable '{col}' con un p-value de {p_value:.4f}.\")\n",
    "            dist = wasserstein_distance(X_train_rest_scaled[col], X_train_last_scaled[col])\n",
    "            print(dist)\n",
    "            if dist > 0.1:  # define tu umbral empírico\n",
    "                print(f\"Data drift en '{col}' con distancia Wasserstein = {dist:.4f}\")\n",
    "                shift_detected = True\n",
    "                training_flag = True                \n",
    "                variables_con_drift.append(f\"Se detecta data drift en la variable '{col}' con una distancia de Wasserstein de {dist:.4f}.\")\n",
    "              \n",
    "                # Plot KDE\n",
    "                #import matplotlib.pyplot as plt\n",
    "                #import seaborn as sns\n",
    "                #plt.figure(figsize=(8, 4))\n",
    "                #sns.kdeplot(X_train_rest[col], label=\"Históricos\", fill=True)\n",
    "                #sns.kdeplot(X_train_last[col], label=\"Último batch\", fill=True)\n",
    "                #plt.title(f\"Drift detectado en '{col}' (p = {dist:.4f})\")\n",
    "                #plt.xlabel(col)\n",
    "                #plt.ylabel(\"Densidad\")\n",
    "                #plt.legend()\n",
    "                #plt.grid(True)\n",
    "                #plt.tight_layout()\n",
    "                #plt.show()\n",
    "                \n",
    "        if shift_detected:\n",
    "            texto_largo = \" \".join(variables_con_drift)\n",
    "            text_validaciones = f\"Existe datadrift en los datos. {texto_largo}. Se entrena nuevo modelo.\"\n",
    "        else:\n",
    "            text_validaciones = \"No existe datadrift en los datos. No se entrena nuevo modelo.\"\n",
    "            test_despliegue = \"No hay nuevo modelo para desplegar\"\n",
    "            \n",
    "if training_flag:\n",
    "    #----------------------------------------Variables de entorno MLFlow--------------------------------\n",
    "    # connects to the Mlflow tracking server that you started above\n",
    "    os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://minio:9000\"\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = 'admin'\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = 'supersecret'\n",
    "    mlflow.set_tracking_uri(\"http://mlflow-service:5000\")\n",
    "      \n",
    "    #---------------------------------------------------------------------------\n",
    "    mlflow.set_experiment(\"mlflow_tracking_houses_prices\")\n",
    "    mlflow.autolog(log_model_signatures=True, log_input_examples=True)\n",
    "    \n",
    "    # run description (just metadata)\n",
    "    desc = \"the simplest possible example\"\n",
    "    \n",
    "    # executes the run\n",
    "    with mlflow.start_run(run_name=\"Regresion_Lineal\", description=\"Modelo de regresión lineal\") as run:\n",
    "        # Entrenar RandomForest\n",
    "        # Modelo\n",
    "        lr_model = LinearRegression()\n",
    "        lr_model.fit(X_train, y_train)\n",
    "        # Predicción\n",
    "        y_pred = lr_model.predict(X_test)\n",
    "        # Calcular accuracy\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mlflow.log_metric(\"rmse_test\", rmse)\n",
    "        mlflow.log_metric(\"modelo_entrenado_en_batch\", max_batch)\n",
    "        \n",
    "    client = MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(\"mlflow_tracking_houses_prices\")\n",
    "    experiment_id = experiment.experiment_id\n",
    "    \n",
    "    # Obtener el modelo registrado más reciente\n",
    "    model_name = \"Regresion_Lineal_modelo_produccion\"\n",
    "    current_run_id = run.info.run_id\n",
    "    \n",
    "    if max_batch==0:\n",
    "        # No hay modelo en producción → registrar y poner este\n",
    "        model_uri = f\"runs:/{current_run_id}/model\"\n",
    "        mv = mlflow.register_model(model_uri, model_name)\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=mv.version,\n",
    "            stage=\"Production\",\n",
    "            archive_existing_versions=True\n",
    "        )\n",
    "        test_despliegue = f\"Se despliega primer modelo con RMSE igual a {rmse:.4f}.\"\n",
    "    \n",
    "    else:\n",
    "        # 1. Obtener la versión actual en producción\n",
    "        production_versions = client.get_latest_versions(model_name, stages=[\"Production\"])\n",
    "        # Hay un modelo en producción → comparar MSE\n",
    "        prod_run_id = production_versions[0].run_id\n",
    "        #prod_metrics = client.get_run(prod_run_id).data.metrics\n",
    "        #rmse_prod = prod_metrics.get(\"rmse_test\")\n",
    "        prod_model_uri = f\"models:/{model_name}/Production\" #Cargar el modelo en producción usando su URI\n",
    "        prod_model = mlflow.pyfunc.load_model(prod_model_uri)\n",
    "        y_pred_prod = prod_model.predict(X_test)#Usar el modelo en producción para predecir el nuevo test set\n",
    "        rmse_prod = np.sqrt(mean_squared_error(y_test, y_pred_prod))#Calcular RMSE con los nuevos datos\n",
    "    \n",
    "        if rmse < rmse_prod:\n",
    "            # El nuevo es mejor → registrar y promover\n",
    "            model_uri = f\"runs:/{current_run_id}/model\"\n",
    "            mv = mlflow.register_model(model_uri, model_name)\n",
    "            client.transition_model_version_stage(\n",
    "                name=model_name,\n",
    "                version=mv.version,\n",
    "                stage=\"Production\",\n",
    "                archive_existing_versions=True\n",
    "            )\n",
    "            test_despliegue = f\"Se registró y desplegó nuevo modelo con RMSE {rmse:.4f}, mejor que el anterior evaluado sobre el nuevo test ({rmse_prod:.4f}).\"\n",
    "        else:\n",
    "            test_despliegue = f\"El nuevo modelo tiene RMSE {rmse:.4f}, peor que el modelo en producción evaluado con el nuevo test ({rmse_prod:.4f}). No se despliega.\"\n",
    "\n",
    "        \n",
    "nueva_fila = pd.DataFrame([{\"batch_number\": max_batch, \"Validaciones-hechas\": text_validaciones, \"Despliegue_de_nuevo_modelo\": test_despliegue, \"Fecha\": datetime.now()}])\n",
    "Validations = pd.concat([Validations, nueva_fila], ignore_index=True)\n",
    "Validations.to_sql(\"validations\", con=engine, index=False, if_exists=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e9e99-6c16-463e-92b4-c1f931685e1c",
   "metadata": {},
   "source": [
    "## TASK 4: Correr método de API para que actualice el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "36891ef3-eb3a-42f9-bd27-d4589f0d849f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error al recargar el modelo: 404 {\"detail\":\"Not Found\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL de tu API (ajusta según puerto y host donde corre FastAPI)\n",
    "url = \"http://api-service:8989/reload-model\"\n",
    "\n",
    "# Realiza la petición POST\n",
    "response = requests.post(url)\n",
    "\n",
    "# Verifica el resultado\n",
    "if response.status_code == 200:\n",
    "    print(\"✅ Recarga exitosa:\", response.json())\n",
    "else:\n",
    "    print(\"❌ Error al recargar el modelo:\", response.status_code, response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
